{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import csv\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# CONV1_SIZE=5\n",
    "# CONV1_KERNEL_NUM=32\n",
    "# CONV2_SIZE=5\n",
    "# CONV2_KERNEL_NUM=64\n",
    "\n",
    "#LENGTH_OF_SEQUENCE\n",
    "LENGTH=16\n",
    "NUM_CHANNELS=1\n",
    "CONV1_SIZE=1\n",
    "CONV1_WIDTH=2\n",
    "CONV1_KERNEL_NUM=1\n",
    "CONV2_SIZE=1\n",
    "CONV2_WIDTH=2\n",
    "CONV2_KERNEL_NUM=1\n",
    "FC_SIZE=512\n",
    "OUTPUT_NODE=3\n",
    "INPUT_NODE=784\n",
    "BATCH_SIZE=210\n",
    "\n",
    "LEARNING_RATE_BASE=0.005\n",
    "LEARNING_RATE_DECAY=0.99\n",
    "REGULARIZER=0.0001\n",
    "STEPS=50000\n",
    "MOVING_AVERAGE_DECAY=0.99\n",
    "MODEL_SAVE_PATH=\"./model/\"\n",
    "MODEL_NAME=\"1d_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20631\n"
     ]
    }
   ],
   "source": [
    "#数据读取\n",
    "file='../data/train_FD001.csv'\n",
    "data=[]\n",
    "with open(file,'r',encoding='utf-8') as csvFile:\n",
    "        reader = csv.reader(csvFile)\n",
    "        for line in reader:\n",
    "            data.append(line)\n",
    "# file=open(path,'r')\n",
    "# data=file.readline()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20631 26 [ 1.00000e+00  1.00000e+00 -7.00000e-04 -4.00000e-04  1.00000e+02\n",
      "  5.18670e+02  6.41820e+02  1.58970e+03  1.40060e+03  1.46200e+01\n",
      "  2.16100e+01  5.54360e+02  2.38806e+03  9.04619e+03  1.30000e+00\n",
      "  4.74700e+01  5.21660e+02  2.38802e+03  8.13862e+03  8.41950e+00\n",
      "  3.00000e-02  3.92000e+02  2.38800e+03  1.00000e+02  3.90600e+01\n",
      "  2.34190e+01]\n"
     ]
    }
   ],
   "source": [
    "#数据处理，处理后data_all的每一个元素都是一个列表，装着每一行数据\n",
    "data_all=[]\n",
    "for i in range(len(data)):\n",
    "    temp=data[i][0].split(' ')[0:26]\n",
    "    data_all.append(temp)\n",
    "data_all=np.array(data_all)\n",
    "data_all=data_all.astype(np.float)\n",
    "print(len(data_all),len(data_all[0]),data_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#获取每个id对应的rul\n",
    "rul=[]\n",
    "for i in range(1,len(data_all)):\n",
    "    if(data_all[i][0]!=data_all[i-1][0]):\n",
    "        rul.append(data_all[i-1][1])\n",
    "rul.append(data_all[len(data_all)-1][1])\n",
    "print(len(rul))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6306\n"
     ]
    }
   ],
   "source": [
    "#下采样（因为normal类数量过多，我这里先对他进行下采样，减少他对整体的影响。remain是对每个id的采样数量，后期你可以自己改）\n",
    "data_new=[]\n",
    "left=0\n",
    "right=192\n",
    "remain=30\n",
    "for t in range(100):\n",
    "    step=int((right-left-30)/remain)\n",
    "    for j in range(left,right-30,step):\n",
    "        data_new.append(data_all[j])\n",
    "    for j in range(right-30,right):\n",
    "        data_new.append(data_all[j])\n",
    "    if(t!=99):\n",
    "        left+=int(rul[t])\n",
    "        right+=int(rul[t+1])\n",
    "print(len(data_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6306\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Critical – If RUL (Remaining Useful Life) is less than 15 Hrs/15 \n",
    "Cycles \n",
    "Warning – If RUL (Remaining Useful Life) is less than 30 Hrs/30\n",
    "Cycles \n",
    "Normal- If RUL (Remaining Useful Life is) more than 30 Hrs/30 Cycles\n",
    "'''\n",
    "#按照上面的注释制作标签，critical对应2，warning对应1，normal对应0\n",
    "label=[]\n",
    "y_all=[]\n",
    "for i in range(len(data_new)):\n",
    "    count=int(data_new[i][0])\n",
    "    if rul[count-1]-data_new[i][1]<15:\n",
    "        label.append('Critical')\n",
    "        y_all.append(2)\n",
    "    elif rul[count-1]-data_new[i][1]<30:\n",
    "        label.append('Warning')\n",
    "        y_all.append(1)\n",
    "    else:\n",
    "        label.append('Normal')\n",
    "        y_all.append(0)\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(label[0:200],y_all[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.00000e-04 -4.00000e-04  6.41820e+02  1.58970e+03  1.40060e+03\n",
      "  5.54360e+02  2.38806e+03  9.04619e+03  4.74700e+01  5.21660e+02\n",
      "  2.38802e+03  8.13862e+03  8.41950e+00  3.92000e+02  3.90600e+01\n",
      "  2.34190e+01] [-4.30000e-03 -1.00000e-04  6.42100e+02  1.58447e+03  1.39837e+03\n",
      "  5.54670e+02  2.38802e+03  9.04968e+03  4.71600e+01  5.21680e+02\n",
      "  2.38803e+03  8.13285e+03  8.41080e+00  3.91000e+02  3.89800e+01\n",
      "  2.33669e+01] [47.47 47.16 47.15 ... 48.09 48.39 48.2 ]\n",
      "[-0.0433526  -0.33333333 -0.33128834 -0.09319817 -0.24264439  0.27094017\n",
      " -0.26923077 -0.390245   -0.13473054  0.13326226 -0.34375    -0.3003922\n",
      " -0.14950884 -0.22727273  0.28632479  0.24534867]\n"
     ]
    }
   ],
   "source": [
    "#我发现有好多列数据，从开始到最后都没有变过，因此我只取了发生变化的那几列（也许不变的数据也有用吧，只是我现在没有用到）\n",
    "#之后是求均值和方差，归一化用\n",
    "avg=[]\n",
    "var=[]\n",
    "data_new=np.array(data_new)\n",
    "x_all=np.hstack((data_new[:,2:4],data_new[:,6:9],data_new[:,11:14],data_new[:,15:20],data_new[:,21:22],data_new[:,24:26]))\n",
    "\n",
    "x_all=np.array(x_all)\n",
    "x_min=np.amin(x_all,axis=0)\n",
    "x_max=np.amax(x_all,axis=0)\n",
    "# print(len(x_min),len(x_max))\n",
    "print(x_all[0],x_all[1],x_all[:,8])\n",
    "x_all=(x_all-x_min)/(x_max-x_min)\n",
    "x_all=x_all-0.5\n",
    "print(x_all[0])\n",
    "# for i in range(17):\n",
    "#     temp=x_all[:,i]\n",
    "#     avg.append(np.mean(temp))\n",
    "#     var.append(np.var(temp))\n",
    "# print(len(avg),len(var))\n",
    "# print(x_all[0],avg,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y_label=np.zeros([len(y_all),3])\n",
    "for i in range(len(y_all)):\n",
    "    y_label[i][y_all[i]]=1\n",
    "x_all=np.array(x_all)\n",
    "print(y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络结构，输入数据为[BATCH_SIZE, LENGTH_OF_SEQUENCE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化w\n",
    "def get_weight(shape,regularizer):\n",
    "    w=tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "    if regularizer != None: tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "#初始化b\n",
    "def get_bias(shape):\n",
    "    b=tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "def conv2d(x,w):\n",
    "    #tf只能输入四维张量，所以把数据扩张为四维\n",
    "    input_3d = tf.expand_dims(x, 1)\n",
    "#     input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    convolution_output=tf.nn.conv2d(input_4d,w,strides=[1,1,1,1],padding='SAME')\n",
    "    #计算完成之后再压缩回二维\n",
    "    conv_output_1d = tf.squeeze(convolution_output)\n",
    "    return conv_output_1d\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    input_3d = tf.expand_dims(x, 1)\n",
    "#     input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    pool_output=tf.nn.max_pool(input_4d,ksize=[1,1,2,1],strides=[1,1,2,1],padding='SAME')\n",
    "    pool_output_1d = tf.squeeze(pool_output)\n",
    "    return pool_output_1d\n",
    "\n",
    "#前向传播\n",
    "def forward(x,train,regularizer):\n",
    "    conv1_w=get_weight([CONV1_SIZE,CONV1_WIDTH,NUM_CHANNELS,CONV1_KERNEL_NUM],regularizer)\n",
    "    conv1_b=get_bias([CONV1_KERNEL_NUM])\n",
    "    conv1=conv2d(x,conv1_w)\n",
    "    input_3d = tf.expand_dims(conv1, 1)\n",
    "#     input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    relu1=tf.nn.relu(tf.nn.bias_add(input_4d,conv1_b))\n",
    "    relu1=tf.squeeze(relu1)\n",
    "    pool1=max_pool_2x2(relu1)\n",
    "    \n",
    "    conv2_w=get_weight([CONV2_SIZE,CONV2_WIDTH,CONV1_KERNEL_NUM,CONV2_KERNEL_NUM],regularizer)\n",
    "    conv2_b=get_bias([CONV2_KERNEL_NUM])\n",
    "    conv2=conv2d(pool1,conv2_w)\n",
    "    input_3d = tf.expand_dims(conv2, 1)\n",
    "#     input_3d = tf.expand_dims(input_2d, 0)\n",
    "    input_4d = tf.expand_dims(input_3d, 3)\n",
    "    relu2=tf.nn.relu(tf.nn.bias_add(input_4d,conv2_b))\n",
    "    relu2=tf.squeeze(relu2)\n",
    "    pool2=max_pool_2x2(relu2)\n",
    "    \n",
    "    pool_shape=pool2.get_shape().as_list()\n",
    "    print(pool_shape)\n",
    "    nodes=pool_shape[1]\n",
    "    reshaped=tf.reshape(pool2,[pool_shape[0],nodes])\n",
    "    \n",
    "    fc1_w=get_weight([nodes,FC_SIZE],regularizer)\n",
    "    fc1_b=get_bias([FC_SIZE])\n",
    "    fc1=tf.nn.relu(tf.matmul(reshaped,fc1_w)+fc1_b)\n",
    "    if train: fc1=tf.nn.dropout(fc1,0.5)\n",
    "    \n",
    "    fc2_w=get_weight([FC_SIZE,OUTPUT_NODE],regularizer)\n",
    "    fc2_b=get_bias([OUTPUT_NODE])\n",
    "    fc2=tf.matmul(fc1,fc2_w)+fc2_b\n",
    "    \n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[210, 4]\n",
      "After 1 training steps, loss on training batch is 1.17385. the acc is 0.633333\n",
      "After 1001 training steps, loss on training batch is 1.05741. the acc is 0.571429\n",
      "After 2001 training steps, loss on training batch is 1.07752. the acc is 0.528571\n",
      "After 3001 training steps, loss on training batch is 0.981813. the acc is 0.571429\n",
      "After 4001 training steps, loss on training batch is 0.849218. the acc is 0.704762\n",
      "After 5001 training steps, loss on training batch is 0.674928. the acc is 0.742857\n",
      "After 6001 training steps, loss on training batch is 0.516142. the acc is 0.819048\n",
      "After 7001 training steps, loss on training batch is 0.509341. the acc is 0.828571\n",
      "After 8001 training steps, loss on training batch is 0.536271. the acc is 0.804762\n",
      "After 9001 training steps, loss on training batch is 0.431186. the acc is 0.847619\n",
      "After 10001 training steps, loss on training batch is 0.446601. the acc is 0.833333\n",
      "After 11001 training steps, loss on training batch is 0.494176. the acc is 0.819048\n",
      "After 12001 training steps, loss on training batch is 0.407774. the acc is 0.861905\n",
      "After 13001 training steps, loss on training batch is 0.432729. the acc is 0.866667\n",
      "After 14001 training steps, loss on training batch is 0.498086. the acc is 0.804762\n",
      "After 15001 training steps, loss on training batch is 0.401662. the acc is 0.861905\n",
      "After 16001 training steps, loss on training batch is 0.417977. the acc is 0.857143\n",
      "After 17001 training steps, loss on training batch is 0.497025. the acc is 0.819048\n",
      "After 18001 training steps, loss on training batch is 0.38695. the acc is 0.871429\n",
      "After 19001 training steps, loss on training batch is 0.425112. the acc is 0.871429\n",
      "After 20001 training steps, loss on training batch is 0.483305. the acc is 0.819048\n",
      "After 21001 training steps, loss on training batch is 0.394156. the acc is 0.866667\n",
      "After 22001 training steps, loss on training batch is 0.418972. the acc is 0.857143\n",
      "After 23001 training steps, loss on training batch is 0.492306. the acc is 0.819048\n",
      "After 24001 training steps, loss on training batch is 0.401215. the acc is 0.847619\n",
      "After 25001 training steps, loss on training batch is 0.432375. the acc is 0.833333\n",
      "After 26001 training steps, loss on training batch is 0.488559. the acc is 0.828571\n",
      "After 27001 training steps, loss on training batch is 0.399341. the acc is 0.866667\n",
      "After 28001 training steps, loss on training batch is 0.406916. the acc is 0.838095\n",
      "After 29001 training steps, loss on training batch is 0.486466. the acc is 0.838095\n",
      "After 30001 training steps, loss on training batch is 0.387865. the acc is 0.871429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-89944b5c3cc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-85-89944b5c3cc0>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-89944b5c3cc0>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After %d training steps, loss on training batch is %g. the acc is %f\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_SAVE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1492\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[0;32m   1493\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices)\u001b[0m\n\u001b[0;32m   1520\u001b[0m     return export_meta_graph(\n\u001b[0;32m   1521\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1522\u001b[1;33m         \u001b[0mgraph_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1523\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[1;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[0;32m   2359\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mwould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mtoo\u001b[0m \u001b[0mlarge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2360\u001b[0m     \"\"\"\n\u001b[1;32m-> 2361\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2362\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[1;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[0;32m   2315\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfrom_version\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mop_id\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mfrom_version\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m           \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;34m\"_output_shapes\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36mextend\u001b[1;34m(self, elem_seq)\u001b[0m\n\u001b[0;32m    411\u001b[0m       \u001b[0mnew_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m       \u001b[0mnew_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SetListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m       \u001b[0mnew_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[0mlistener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModified\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m   1316\u001b[0m           \u001b[0mfield_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m           \u001b[0mfields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1318\u001b[1;33m         \u001b[0mfield_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_present_in_parent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36mMergeFrom\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mMergeFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m       \u001b[1;31m# According to documentation: \"When parsing from the wire or when merging,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       \u001b[1;31m# if there are duplicate map keys the last key seen is used\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\google\\protobuf\\internal\\containers.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def backward():\n",
    "    x=tf.placeholder(\n",
    "        tf.float32,[\n",
    "            BATCH_SIZE,\n",
    "            LENGTH\n",
    "        ]\n",
    "    )\n",
    "    y_=tf.placeholder(tf.float32,[None,OUTPUT_NODE])\n",
    "    y=forward(x,True,REGULARIZER)\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    \n",
    "    ce=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_,1))\n",
    "    cem=tf.reduce_mean(ce)\n",
    "    loss=cem + tf.add_n(tf.get_collection('losses'))\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    learning_rate=tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        200,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True\n",
    "    )\n",
    "    \n",
    "    train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)\n",
    "    ema_op=ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([train_step,ema_op]):\n",
    "        train_op=tf.no_op(name='train')\n",
    "        \n",
    "    saver=tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_op=tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for i in range(STEPS):\n",
    "            nn=i%30\n",
    "            #输入数据为x_all,标签为y_label，每次取一个batch的数据训练\n",
    "            xs=x_all[nn*BATCH_SIZE:(nn+1)*BATCH_SIZE]\n",
    "            ys=y_label[nn*BATCH_SIZE:(nn+1)*BATCH_SIZE]\n",
    "#             xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "            reshaped_xs=np.reshape(xs,(\n",
    "                BATCH_SIZE,\n",
    "                LENGTH\n",
    "            ))\n",
    "            _,loss_value,step,acc=sess.run([train_op,loss,global_step,accuracy], feed_dict={x:reshaped_xs,y_:ys})\n",
    "            if i%1000 == 0:\n",
    "                print(\"After %d training steps, loss on training batch is %g. the acc is %f\"% (step, loss_value,acc))\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step=global_step)\n",
    "                \n",
    "def main():\n",
    "    backward()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 3]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x=np.random.rand(100,10)\n",
    "x=x.astype(np.float32)\n",
    "# x=tf.placeholder(tf.float32,[1,1,25,1])\n",
    "# print(forward(x,True,REGULARIZER))\n",
    "x_input_1d=tf.placeholder(tf.float32,[100,10])\n",
    "y=forward(x_input_1d,True,REGULARIZER)\n",
    "feed_dict = {x_input_1d: x}\n",
    "with tf.Session() as sess:\n",
    "        init_op=tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        print(sess.run(y, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
